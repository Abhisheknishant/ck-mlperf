{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [dividiti](http://dividiti.com)'s submissions to [MLPerf Inference v0.5](https://github.com/mlperf/inference/tree/master/v0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"overview\"></a>\n",
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [General MLPerf Submission Rules](https://github.com/mlperf/policies/blob/master/submission_rules.adoc)\n",
    "- [MLPerf Inference Rules](https://github.com/mlperf/inference_policies/blob/master/inference_rules.adoc)\n",
    "- [`submission_checker.py`](https://github.com/mlperf/inference/blob/master/v0.5/tools/submission/submission-checker.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"systems\"></a>\n",
    "## Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Firefly-RK3399"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firefly_tflite = {\n",
    "    \"division\": \"open\",\n",
    "    \"submitter\": \"dividiti\",\n",
    "    \"status\": \"available\",\n",
    "    \"system_name\": \"Firefly-RK3399\",\n",
    "\n",
    "    \"number_of_nodes\": \"1\",\n",
    "    \"host_processor_model_name\": \"Arm Cortex-A73 (big), Arm Cortex-A53 (LITTLE)\",\n",
    "    \"host_processors_per_node\": \"1\",\n",
    "    \"host_processor_core_count\": \"2 (big), 4 (LITTLE)\",\n",
    "    \"host_processor_frequency\": \"1800 MHz (big), 1400 MHz (LITTLE)\",\n",
    "    \"host_processor_caches\": \"N/A\",\n",
    "    \"host_memory_configuration\": \"?\",\n",
    "    \"host_memory_capacity\": \"4 GB\",\n",
    "    \"host_storage_capacity\": \"128 GB\",\n",
    "    \"host_storage_type\": \"microSD\",\n",
    "    \"host_processor_interconnect\": \"N/A\",\n",
    "    \"host_networking\": \"N/A\",\n",
    "    \"host_networking_topology\": \"N/A\",\n",
    "\n",
    "    \"accelerators_per_node\": \"1\",\n",
    "    \"accelerator_model_name\": \"Arm Mali-T860 MP4\",\n",
    "    \"accelerator_frequency\": \"800 MHz\",\n",
    "    \"accelerator_host_interconnect\": \"N/A\",\n",
    "    \"accelerator_interconnect\": \"N/A\",\n",
    "    \"accelerator_interconnect_topology\": \"N/A\",\n",
    "    \"accelerator_memory_capacity\": \"4 GB (shared)\",\n",
    "    \"accelerator_memory_configuration\": \"?\",\n",
    "    \"accelerator_on-chip_memories\": \"N/A\",\n",
    "    \"cooling\": \"on-board fan\",\n",
    "    \"hw_notes\": \"http://en.t-firefly.com/product/rk3399/\",\n",
    "\n",
    "    \"framework\": \"TFLite-v1.15-rc2\",\n",
    "    \"operating_system\": \"Ubuntu 16.04.6 LTS\",\n",
    "    \"other_software_stack\": \"Collective Knowledge v1.11.1\",\n",
    "    \"sw_notes\": \"Powered by CK\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "systems = {\n",
    "    'firefly-tflite-v1.15' : firefly_tflite\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_desc_id.json\n",
    "default_system_desc_id = {\n",
    "    \"division\": \"reqired\",\n",
    "    \"submitter\": \"required\",\n",
    "    \"status\": \"required\",\n",
    "    \"system_name\": \"required\",\n",
    "\n",
    "    \"number_of_nodes\": \"required\",\n",
    "    \"host_processor_model_name\": \"required\",\n",
    "    \"host_processors_per_node\": \"required\",\n",
    "    \"host_processor_core_count\": \"required\",\n",
    "    \"host_processor_frequency\": \"\",\n",
    "    \"host_processor_caches\": \"\",\n",
    "    \"host_memory_configuration\": \"\",\n",
    "    \"host_memory_capacity\": \"required\",\n",
    "    \"host_storage_capacity\": \"required\",\n",
    "    \"host_storage_type\": \"required\",\n",
    "    \"host_processor_interconnect\": \"\",\n",
    "    \"host_networking\": \"\",\n",
    "    \"host_networking_topology\": \"\",\n",
    "\n",
    "    \"accelerators_per_node\": \"required\",\n",
    "    \"accelerator_model_name\": \"required\",\n",
    "    \"accelerator_frequency\": \"\",\n",
    "    \"accelerator_host_interconnect\": \"\",\n",
    "    \"accelerator_interconnect\": \"\",\n",
    "    \"accelerator_interconnect_topology\": \"\",\n",
    "    \"accelerator_memory_capacity\": \"required\",\n",
    "    \"accelerator_memory_configuration\": \"\",\n",
    "    \"accelerator_on-chip_memories\": \"\",\n",
    "    \"cooling\": \"\",\n",
    "    \"hw_notes\": \"\",\n",
    "\n",
    "    \"framework\": \"required\",\n",
    "    \"operating_system\": \"required\",\n",
    "    \"other_software_stack\": \"required\",\n",
    "    \"sw_notes\": \"\"\n",
    "}\n",
    "\n",
    "# system_desc_id_imp.json\n",
    "default_system_desc_id_tmp = {\n",
    "    \"input_data_types\": \"required\",\n",
    "    \"retraining\": \"required\",\n",
    "    \"starting_weights_filename\": \"required\",\n",
    "    \"weight_data_types\": \"required\",\n",
    "    \"weight_transformations\": \"required\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>\n",
    "## Get the experimental data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"code\"></a>\n",
    "## Data wrangling code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** Please ignore this section if you are not interested in re-running or modifying this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Includes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scientific"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If some of the scientific packages are missing, please install them using:\n",
    "```\n",
    "# pip install jupyter pandas numpy matplotlib\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython as ip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('IPython version: %s' % ip.__version__)\n",
    "print ('Pandas version: %s' % pd.__version__)\n",
    "print ('NumPy version: %s' % np.__version__)\n",
    "print ('Matplotlib version: %s' % mp.__version__)\n",
    "print ('Seaborn version: %s' % sb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "def display_in_full(df):\n",
    "    pd.options.display.max_columns = len(df.columns)\n",
    "    pd.options.display.max_rows = len(df.index)\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_colormap = cm.autumn\n",
    "default_fontsize = 16\n",
    "default_barwidth = 0.8\n",
    "default_figwidth = 24\n",
    "default_figheight = 3\n",
    "default_figdpi = 200\n",
    "default_figsize = [default_figwidth, default_figheight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mp.__version__[0]=='2': mp.style.use('classic')\n",
    "mp.rcParams['figure.max_open_warning'] = 200\n",
    "mp.rcParams['figure.dpi'] = default_figdpi\n",
    "mp.rcParams['font.size'] = default_fontsize\n",
    "mp.rcParams['legend.fontsize'] = 'medium'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fig_dir = os.path.join(os.path.expanduser(\"~\"), 'mlperf-dividiti')\n",
    "if not os.path.exists(save_fig_dir):\n",
    "    os.mkdir(save_fig_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collective Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If CK is not installed, please install it using:\n",
    "```\n",
    "# python -m pip install ck\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ck.kernel as ck\n",
    "print ('CK version: %s' % ck.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimental data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download experimental data and add CK repositories as follows:\n",
    "```\n",
    "$ wget https://www.dropbox.com/s/<...>/mlperf.*.zip\n",
    "$ ck add repo --zip=mlperf.*.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ck recache repo\n",
    "repo_uoa = 'mlperf.open.image-classification.firefly'\n",
    "!ck list $repo_uoa:experiment:* | sort\n",
    "print (\"*****************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experimental_results(repo_uoa, module_uoa='experiment', tags='mlperf', path=None, submitter='dividiti'):\n",
    "    if not path:\n",
    "        path_list = !ck find repo:$repo_uoa\n",
    "        path = path_list[0]\n",
    "    print(\"Storing results under '%s' ...\\n\" % path)\n",
    "    \n",
    "    r = ck.access({'action':'search', 'repo_uoa':repo_uoa, 'module_uoa':module_uoa, 'tags':tags})\n",
    "#    pprint (r)\n",
    "    if r['return']>0:\n",
    "        print('Error: %s' % r['error'])\n",
    "        exit(1)\n",
    "    experiments = r['lst']\n",
    "\n",
    "    dfs = []\n",
    "    for experiment in experiments:\n",
    "        data_uoa = experiment['data_uoa']\n",
    "        r = ck.access({'action':'list_points', 'repo_uoa':repo_uoa, 'module_uoa':module_uoa, 'data_uoa':data_uoa})\n",
    "        if r['return']>0:\n",
    "            print('Error: %s' % r['error'])\n",
    "            exit(1)\n",
    "        \n",
    "        tags = r['dict']['tags']\n",
    "        # Expected format: [ \"mlperf\", \"open\", \"image-classification\", \"firefly\", \"tflite-v1.15\", \"mobilenet-v1-0.5-128\", \"singlestream\", \"performance\" ]\n",
    "        if 'accuracy' in tags:\n",
    "            (_, division, task, platform, library, benchmark, scenario, mode, preprocessing) = tags\n",
    "        elif 'performance' in tags:\n",
    "            (_, division, task, platform, library, benchmark, scenario, mode) = tags\n",
    "        organization = submitter\n",
    "        system = platform+'-'+library\n",
    "        \n",
    "        # Directory structure for submissions according to MLPerf Inference Rules:\n",
    "        #\n",
    "        # <division>/\n",
    "        #   <organization>/\n",
    "        #     \"systems\"/\n",
    "        #       <system_desc_id>.json\n",
    "        #     \"results\"/\n",
    "        #       <system_desc_id>/\n",
    "        #         <benchmark>/\n",
    "        #           <scenario>/\n",
    "        #             performance/\n",
    "        #               run_x/ # 1 run for single stream and offline, 5 otherwise\n",
    "        #                 mlperf_log_summary.txt\n",
    "        #                 mlperf_log_detail.txt\n",
    "        #                 mlperf_log_trace.json\n",
    "        #             accuracy/\n",
    "        #               mlperf_log_accuracy.json\n",
    "        #   compliance_checker_log.txt\n",
    "        #\n",
    "        root_dir = os.path.join(path, 'submissions_inference_0_5')\n",
    "        if not os.path.exists(root_dir): os.mkdir(root_dir)\n",
    "        division_dir = os.path.join(root_dir, division)\n",
    "        if not os.path.exists(division_dir): os.mkdir(division_dir)\n",
    "        organization_dir = os.path.join(division_dir, organization)\n",
    "        if not os.path.exists(organization_dir): os.mkdir(organization_dir)\n",
    "        # \"systems\"/\n",
    "        systems_dir = os.path.join(organization_dir, 'systems')\n",
    "        if not os.path.exists(systems_dir): os.mkdir(systems_dir)\n",
    "        system_json_name = '%s.json' % system\n",
    "        system_json_path = os.path.join(systems_dir, system_json_name)\n",
    "        with open(system_json_path, 'w') as system_json_file:\n",
    "            json.dump(systems.get(system, default_system_desc_id), system_json_file, indent=2)\n",
    "            print('  |_ %s' % system_json_name)\n",
    "        # \"results\"/\n",
    "        results_dir = os.path.join(organization_dir, 'results')\n",
    "        if not os.path.exists(results_dir): os.mkdir(results_dir)\n",
    "        system_dir = os.path.join(results_dir, system)\n",
    "        if not os.path.exists(system_dir): os.mkdir(system_dir)\n",
    "        benchmark_dir = os.path.join(system_dir, benchmark)\n",
    "        if not os.path.exists(benchmark_dir): os.mkdir(benchmark_dir)\n",
    "        scenario_dir = os.path.join(benchmark_dir, scenario)\n",
    "        if not os.path.exists(scenario_dir): os.mkdir(scenario_dir)\n",
    "        mode_dir = os.path.join(scenario_dir, mode)\n",
    "        if not os.path.exists(mode_dir): os.mkdir(mode_dir)\n",
    "        # For each point (should be one point for each performance run).\n",
    "        points = r['points']\n",
    "        for (point, point_idx) in zip(points, range(1,len(points)+1)):\n",
    "            point_file_path = os.path.join(r['path'], 'ckp-%s.0001.json' % point)\n",
    "            with open(point_file_path) as point_file:\n",
    "                point_data_raw = json.load(point_file)\n",
    "            characteristics_list = point_data_raw['characteristics_list']\n",
    "            characteristics = characteristics_list[0]\n",
    "            # Set the leaf directory.\n",
    "            if mode == 'performance':\n",
    "                run_dir = os.path.join(mode_dir, 'run_%d' % point_idx)\n",
    "                if not os.path.exists(run_dir): os.mkdir(run_dir)\n",
    "                last_dir = run_dir\n",
    "            else:\n",
    "                last_dir = mode_dir\n",
    "            print(last_dir)\n",
    "            # Dump files in the leaf directory.\n",
    "            mlperf_log = characteristics['run'].get('mlperf_log',{})\n",
    "            # Summary file (with errors and warnings).\n",
    "            summary_txt_name = 'mlperf_log_summary.txt'\n",
    "            summary_txt_path = os.path.join(last_dir, summary_txt_name)\n",
    "            with open(summary_txt_path, 'w') as summary_txt_file:\n",
    "                summary_txt_file.writelines(mlperf_log.get('summary',''))\n",
    "                print('  |_ %s' % summary_txt_name)\n",
    "            # Detail file (with settings).\n",
    "            detail_txt_name = 'mlperf_log_detail.txt'\n",
    "            detail_txt_path = os.path.join(last_dir, detail_txt_name)\n",
    "            with open(detail_txt_path, 'w') as detail_txt_file:\n",
    "                detail_txt_file.writelines(mlperf_log.get('detail',''))\n",
    "                print('  |_ %s' % detail_txt_name)\n",
    "            # Accuracy file (with accuracy dictionary).\n",
    "            # FIXME: Move the next 5 lines into the (if mode == 'accuracy') block,\n",
    "            # once the submission checker no longer complains as follows:\n",
    "            # \"performance/run_1 has file list mismatch (['mlperf_log_accuracy.json'])\"\n",
    "            accuracy_json_name = 'mlperf_log_accuracy.json'\n",
    "            accuracy_json_path = os.path.join(last_dir, accuracy_json_name)\n",
    "            with open(accuracy_json_path, 'w') as accuracy_json_file:\n",
    "                json.dump(mlperf_log.get('accuracy',{}), accuracy_json_file)\n",
    "                print('  |_ %s' % accuracy_json_name)\n",
    "            if mode == 'accuracy':\n",
    "                # FIXME: Do not hardcode - locate via CK.\n",
    "                accuracy_imagenet_py = '$HOME/CK_TOOLS/mlperf-inference-upstream.master/inference/v0.5/classification_and_detection/tools/accuracy-imagenet.py'\n",
    "                imagenet_val_file = '$HOME/CK_TOOLS/dataset-imagenet-ilsvrc2012-aux/val.txt'\n",
    "                accuracy_txt_name = 'accuracy.txt'\n",
    "                accuracy_txt_path = os.path.join(last_dir, accuracy_txt_name)\n",
    "                accuracy_txt = !python3 $accuracy_imagenet_py --imagenet-val-file $imagenet_val_file --mlperf-accuracy-file $accuracy_json_path\n",
    "                with open(accuracy_txt_path, 'w') as accuracy_txt_file:\n",
    "                    accuracy_txt_file.writelines(accuracy_txt)\n",
    "                    # Print the first line containing accuracy info.\n",
    "                    print('  |_ %s (\"%s\")' % (accuracy_txt_name, accuracy_txt[0]))\n",
    "#             # Trace file (an omit trace from v0.5).\n",
    "#             trace_json_name = 'mlperf_log_trace.json'\n",
    "#             trace_json_path = os.path.join(last_dir, trace_json_name)\n",
    "#             with open(trace_json_path, 'w') as trace_json_file:\n",
    "#                 json.dump(mlperf_log.get('trace',{}), trace_json_file)\n",
    "    \n",
    "    # FIXME: Do not hardcode - locate via CK.\n",
    "    submission_checker_py = '$HOME/CK_TOOLS/mlperf-inference-upstream.master/inference/v0.5/tools/submission/submission-checker.py'\n",
    "    !python3 $submission_checker_py --input $root_dir --submitter $submitter\n",
    "    return\n",
    "\n",
    "dfs = get_experimental_results(repo_uoa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot experimental data"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
