{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [dividiti](http://dividiti.com)'s submissions to [MLPerf Inference v0.5](https://github.com/mlperf/inference/tree/master/v0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"overview\"></a>\n",
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook covers [dividiti](http://dividiti.com)'s submissions to [MLPerf Inference v0.5](https://github.com/mlperf/inference/tree/master/v0.5). It validates that experimental data obtained via automated, portable and reproducible [Collective Knowledge](http://cknowledge.org) workflows conforms to [General MLPerf Submission Rules](https://github.com/mlperf/policies/blob/master/submission_rules.adoc)\n",
    "and [MLPerf Inference Rules](https://github.com/mlperf/inference_policies/blob/master/inference_rules.adoc), including runnning the official [`submission_checker.py`](https://github.com/mlperf/inference/blob/master/v0.5/tools/submission/submission-checker.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A live version of this Jupyter Notebook can be viewed [here](https://nbviewer.jupyter.org/urls/dl.dropbox.com/s/1xlv5oacgobrfd4/mlperf-inference-v0.5-dividiti.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Overview](#overview)\n",
    "1. [Includes](#includes)\n",
    "1. [System templates](#templates)\n",
    "  1. [Firefly RK3399](#templates_firefly)\n",
    "  1. [Linaro HiKey960](#templates_hikey960)\n",
    "  1. [Huawei Mate 10 Pro](#templates_mate10pro)\n",
    "  1. [Raspberry Pi 4](#templates_rpi4)\n",
    "  1. [Default](#templates_default)\n",
    "1. [Systems](#systems)\n",
    "1. [Implementations](#implementations)\n",
    "1. [Get the experimental data](#get)\n",
    "  1. [Image Classification - Closed](#get_image_classification_closed)\n",
    "  1. [Image Classification - Open](#get_image_classification_open)\n",
    "1. [Generate the submission checklist](#checklist)\n",
    "1. [Check the experimental data](#check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"includes\"></a>\n",
    "## Includes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "\n",
    "from pprint import pprint\n",
    "from shutil import copy2\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scientific"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If some of the scientific packages are missing, please install them using:\n",
    "```\n",
    "# python3 -m pip install jupyter pandas numpy matplotlib seaborn --user\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython as ip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('IPython version: %s' % ip.__version__)\n",
    "print ('Pandas version: %s' % pd.__version__)\n",
    "print ('NumPy version: %s' % np.__version__)\n",
    "print ('Matplotlib version: %s' % mp.__version__)\n",
    "print ('Seaborn version: %s' % sb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "def display_in_full(df):\n",
    "    pd.options.display.max_columns = len(df.columns)\n",
    "    pd.options.display.max_rows = len(df.index)\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_colormap = cm.autumn\n",
    "default_fontsize = 16\n",
    "default_barwidth = 0.8\n",
    "default_figwidth = 24\n",
    "default_figheight = 3\n",
    "default_figdpi = 200\n",
    "default_figsize = [default_figwidth, default_figheight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mp.__version__[0]=='2': mp.style.use('classic')\n",
    "mp.rcParams['figure.max_open_warning'] = 200\n",
    "mp.rcParams['figure.dpi'] = default_figdpi\n",
    "mp.rcParams['font.size'] = default_fontsize\n",
    "mp.rcParams['legend.fontsize'] = 'medium'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fig_dir = os.path.join(os.path.expanduser(\"~\"), 'mlperf-dividiti')\n",
    "if not os.path.exists(save_fig_dir):\n",
    "    os.mkdir(save_fig_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collective Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If CK is not installed, please install it using:\n",
    "```\n",
    "# python -m pip install ck\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ck.kernel as ck\n",
    "print ('CK version: %s' % ck.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"templates\"></a>\n",
    "## System templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"templates_firefly\"></a>\n",
    "### [Firefly-RK3399](http://en.t-firefly.com/product/rk3399/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firefly = {\n",
    "    \"division\": \"\",\n",
    "    \"submitter\": \"dividiti\",\n",
    "    \"status\": \"available\",\n",
    "    \"system_name\": \"Firefly-RK3399 (firefly)\",\n",
    "\n",
    "    \"number_of_nodes\": \"1\",\n",
    "    \"host_processor_model_name\": \"Arm Cortex-A72 MP2 (big); Arm Cortex-A53 MP4 (LITTLE)\",\n",
    "    \"host_processors_per_node\": \"1\",\n",
    "    \"host_processor_core_count\": \"2 (big); 4 (LITTLE)\",\n",
    "    \"host_processor_frequency\": \"1800 MHz (big), 1400 MHz (LITTLE)\",\n",
    "    \"host_processor_caches\": \"L1I$ 48 KiB, L1D$ 32 KiB, L2$ 1 MiB (big); L1I$ 32 KiB, L1D$ 32 KiB, L2$ 512 KiB (LITTLE)\",\n",
    "    \"host_memory_configuration\": \"-\",\n",
    "    \"host_memory_capacity\": \"4 GiB\",\n",
    "    \"host_storage_capacity\": \"128 GiB\",\n",
    "    \"host_storage_type\": \"SanDisk Extreme microSD\",\n",
    "    \"host_processor_interconnect\": \"-\",\n",
    "    \"host_networking\": \"-\",\n",
    "    \"host_networking_topology\": \"-\",\n",
    "\n",
    "    \"accelerators_per_node\": \"1\",\n",
    "    \"accelerator_model_name\": \"Arm Mali-T860 MP4\",\n",
    "    \"accelerator_frequency\": \"800 MHz\",\n",
    "    \"accelerator_host_interconnect\": \"-\",\n",
    "    \"accelerator_interconnect\": \"-\",\n",
    "    \"accelerator_interconnect_topology\": \"-\",\n",
    "    \"accelerator_memory_capacity\": \"4 GiB (shared with host)\",\n",
    "    \"accelerator_memory_configuration\": \"-\",\n",
    "    \"accelerator_on-chip_memories\": \"-\",\n",
    "    \"cooling\": \"on-board fan\",\n",
    "    \"hw_notes\": \"http://en.t-firefly.com/product/rk3399/; http://opensource.rock-chips.com/wiki_RK3399\",\n",
    "\n",
    "    \"framework\": \"\",\n",
    "    \"operating_system\": \"Ubuntu 16.04.6 LTS; kernel 4.4.77 #554 (Thu Nov 30 11:30:11 HKT 2017)\",\n",
    "    \"other_software_stack\": \"GCC 7.4.0; Python 3.5.2; OpenCL driver 1.2 v1.r13p0-00rel0-git(a4271c9).31ba04af2d3c01618138bef3aed66c2c\",\n",
    "    \"sw_notes\": \"Powered by Collective Knowledge v1.11.1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"templates_hikey960\"></a>\n",
    "### [Linaro HiKey960](https://www.96boards.org/product/hikey960/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hikey960 = {\n",
    "    \"division\": \"\",\n",
    "    \"submitter\": \"dividiti\",\n",
    "    \"status\": \"available\",\n",
    "    \"system_name\": \"Linaro HiKey960 (hikey960)\",\n",
    "\n",
    "    \"number_of_nodes\": \"1\",\n",
    "    \"host_processor_model_name\": \"Arm Cortex-A73 MP4 (big); Arm Cortex-A53 MP4 (LITTLE)\",\n",
    "    \"host_processors_per_node\": \"1\",\n",
    "    \"host_processor_core_count\": \"4 (big); 4 (LITTLE)\",\n",
    "    \"host_processor_frequency\": \"2362 MHz (big), 1844 MHz (LITTLE)\",\n",
    "    \"host_processor_caches\": \"L1I$ 256=4x64 KiB, L1D$ 256=4x64 KiB, L2$ 2 MiB (big); L1I$ 128=4x32 KiB, L1D$ 128=4x32 KiB, L2$ 1 MiB (LITTLE)\",\n",
    "    \"host_memory_configuration\": \"-\",\n",
    "    \"host_memory_capacity\": \"3 GiB\",\n",
    "    \"host_storage_capacity\": \"128 GiB\",\n",
    "    \"host_storage_type\": \"SanDisk Extreme microSD\",\n",
    "    \"host_processor_interconnect\": \"-\",\n",
    "    \"host_networking\": \"-\",\n",
    "    \"host_networking_topology\": \"-\",\n",
    "\n",
    "    \"accelerators_per_node\": \"1\",\n",
    "    \"accelerator_model_name\": \"Arm Mali-G71 MP8\",\n",
    "    \"accelerator_frequency\": \"800 MHz\",\n",
    "    \"accelerator_host_interconnect\": \"-\",\n",
    "    \"accelerator_interconnect\": \"-\",\n",
    "    \"accelerator_interconnect_topology\": \"-\",\n",
    "    \"accelerator_memory_capacity\": \"3 GiB (shared with host)\",\n",
    "    \"accelerator_memory_configuration\": \"-\",\n",
    "    \"accelerator_on-chip_memories\": \"-\",\n",
    "    \"cooling\": \"small external fan\",\n",
    "    \"hw_notes\": \"http://www.hisilicon.com/en/Products/ProductList/Kirin\",\n",
    "\n",
    "    \"framework\": \"\",\n",
    "    \"operating_system\": \"Debian 9; kernel 4.19.5-hikey #26 (Thu Aug 22 07:58:35 UTC 2019)\",\n",
    "    \"other_software_stack\": \"GCC 7.4.0; Python 3.5.3; OpenCL driver 2.0 v1.r16p0\",\n",
    "    \"sw_notes\": \"Powered by Collective Knowledge v1.11.1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"templates_mate10pro\"></a>\n",
    "### Huawei Mate 10 Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mate10pro = {\n",
    "    \"division\": \"\",\n",
    "    \"submitter\": \"dividiti\",\n",
    "    \"status\": \"available\",\n",
    "    \"system_name\": \"Huawei Mate 10 Pro (mate10pro)\",\n",
    "\n",
    "    \"number_of_nodes\": \"1\",\n",
    "    \"host_processor_model_name\": \"Arm Cortex-A73 MP4 (big); Arm Cortex-A53 MP4 (LITTLE)\",\n",
    "    \"host_processors_per_node\": \"1\",\n",
    "    \"host_processor_core_count\": \"4 (big); 4 (LITTLE)\",\n",
    "    \"host_processor_frequency\": \"2360 MHz (big), 1800 MHz (LITTLE)\",\n",
    "    \"host_processor_caches\": \"L1I$ 256=4x64 KiB, L1D$ 256=4x64 KiB, L2$ 2 MiB (big); L1I$ 128=4x32 KiB, L1D$ 128=4x32 KiB, L2$ 1 MiB (LITTLE)\",\n",
    "    \"host_memory_configuration\": \"-\",\n",
    "    \"host_memory_capacity\": \"6 GiB\",\n",
    "    \"host_storage_capacity\": \"128 GiB\",\n",
    "    \"host_storage_type\": \"Flash\",\n",
    "    \"host_processor_interconnect\": \"-\",\n",
    "    \"host_networking\": \"-\",\n",
    "    \"host_networking_topology\": \"-\",\n",
    "\n",
    "    \"accelerators_per_node\": \"1\",\n",
    "    \"accelerator_model_name\": \"Arm Mali-G72 MP12\",\n",
    "    \"accelerator_frequency\": \"850 MHz\",\n",
    "    \"accelerator_host_interconnect\": \"-\",\n",
    "    \"accelerator_interconnect\": \"-\",\n",
    "    \"accelerator_interconnect_topology\": \"-\",\n",
    "    \"accelerator_memory_capacity\": \"6 GiB (shared with host)\",\n",
    "    \"accelerator_memory_configuration\": \"-\",\n",
    "    \"accelerator_on-chip_memories\": \"-\",\n",
    "    \"cooling\": \"phone case\",\n",
    "    \"hw_notes\": \"https://en.wikichip.org/wiki/hisilicon/kirin/970\",\n",
    "\n",
    "    \"framework\": \"\",\n",
    "    \"operating_system\": \"Android 9.1.0.300(C782E5R1P11); kernel 4.9.148 (Sat Jun 29 20:41:06 CST 2019)\",\n",
    "    \"other_software_stack\": \"Android NDK 17c (LLVM 6.0.2); OpenCL driver 2.0 v1.r14p0-00cet0.0416641283c5d6e2d53c163d0ca99357\",\n",
    "    \"sw_notes\": \"Powered by Collective Knowledge v1.11.1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"templates_rpi4\"></a>\n",
    "### Raspberry Pi 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpi4 = {\n",
    "    \"division\": \"\",\n",
    "    \"submitter\": \"dividiti\",\n",
    "    \"status\": \"available\",\n",
    "    \"system_name\": \"Raspberry Pi 4 (rpi4)\",\n",
    "\n",
    "    \"number_of_nodes\": \"1\",\n",
    "    \"host_processor_model_name\": \"Arm Cortex-A72 MP4\",\n",
    "    \"host_processors_per_node\": \"1\",\n",
    "    \"host_processor_core_count\": \"4\",\n",
    "    \"host_processor_frequency\": \"1500 MHz\",\n",
    "    \"host_processor_caches\": \"L1I$ 128=4x32 KiB, L1D$ 128=4x32 KiB, L2$ 1 MiB\",\n",
    "    \"host_memory_configuration\": \"-\",\n",
    "    \"host_memory_capacity\": \"4 GiB\",\n",
    "    \"host_storage_capacity\": \"128 GiB\",\n",
    "    \"host_storage_type\": \"SanDisk Extreme Pro microSD\",\n",
    "    \"host_processor_interconnect\": \"-\",\n",
    "    \"host_networking\": \"-\",\n",
    "    \"host_networking_topology\": \"-\",\n",
    "\n",
    "    \"accelerators_per_node\": \"0\",\n",
    "    \"accelerator_model_name\": \"-\",\n",
    "    \"accelerator_frequency\": \"-\",\n",
    "    \"accelerator_host_interconnect\": \"-\",\n",
    "    \"accelerator_interconnect\": \"-\",\n",
    "    \"accelerator_interconnect_topology\": \"-\",\n",
    "    \"accelerator_memory_capacity\": \"-\",\n",
    "    \"accelerator_memory_configuration\": \"-\",\n",
    "    \"accelerator_on-chip_memories\": \"-\",\n",
    "    \"cooling\": \"http://www.raspberrypiwiki.com/index.php/Armor_Case_B\",\n",
    "    \"hw_notes\": \"https://www.raspberrypi.org/products/raspberry-pi-4-model-b/specifications/\",\n",
    "\n",
    "    \"framework\": \"\",\n",
    "    \"operating_system\": \"Raspbian Buster (Debian 10); kernel 4.19.66-v7l+ #1253 (Thu Aug 15 12:02:08 BST 2019)\",\n",
    "    \"other_software_stack\": \"GCC 8.3.0; Python 3.7.3\",\n",
    "    \"sw_notes\": \"Powered by Collective Knowledge v1.11.1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"templates_default\"></a>\n",
    "### Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default `system_desc_id.json` (to catch uninitialized descriptions)\n",
    "default_system_json = {\n",
    "    \"division\": \"reqired\",\n",
    "    \"submitter\": \"required\",\n",
    "    \"status\": \"required\",\n",
    "    \"system_name\": \"required\",\n",
    "\n",
    "    \"number_of_nodes\": \"required\",\n",
    "    \"host_processor_model_name\": \"required\",\n",
    "    \"host_processors_per_node\": \"required\",\n",
    "    \"host_processor_core_count\": \"required\",\n",
    "    \"host_processor_frequency\": \"\",\n",
    "    \"host_processor_caches\": \"\",\n",
    "    \"host_memory_configuration\": \"\",\n",
    "    \"host_memory_capacity\": \"required\",\n",
    "    \"host_storage_capacity\": \"required\",\n",
    "    \"host_storage_type\": \"required\",\n",
    "    \"host_processor_interconnect\": \"\",\n",
    "    \"host_networking\": \"\",\n",
    "    \"host_networking_topology\": \"\",\n",
    "\n",
    "    \"accelerators_per_node\": \"required\",\n",
    "    \"accelerator_model_name\": \"required\",\n",
    "    \"accelerator_frequency\": \"\",\n",
    "    \"accelerator_host_interconnect\": \"\",\n",
    "    \"accelerator_interconnect\": \"\",\n",
    "    \"accelerator_interconnect_topology\": \"\",\n",
    "    \"accelerator_memory_capacity\": \"required\",\n",
    "    \"accelerator_memory_configuration\": \"\",\n",
    "    \"accelerator_on-chip_memories\": \"\",\n",
    "    \"cooling\": \"\",\n",
    "    \"hw_notes\": \"\",\n",
    "\n",
    "    \"framework\": \"required\",\n",
    "    \"operating_system\": \"required\",\n",
    "    \"other_software_stack\": \"required\",\n",
    "    \"sw_notes\": \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"systems\"></a>\n",
    "## Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate division_systems dictionary.\n",
    "division_systems = {}\n",
    "\n",
    "platform_templates = {\n",
    "    'firefly'   : firefly,\n",
    "    'hikey960'  : hikey960,\n",
    "    'mate10pro' : mate10pro,\n",
    "    'rpi4' : rpi4,\n",
    "}\n",
    "\n",
    "divisions = [ 'open', 'closed' ]\n",
    "platforms = [ 'firefly', 'hikey960', 'mate10pro', 'rpi4' ]\n",
    "for division in divisions:\n",
    "    for platform in platforms:\n",
    "        if platform == 'mate10pro':\n",
    "            libraries = [ 'tflite-v1.13', 'armnn-v19.08' ]\n",
    "        else:\n",
    "            libraries = [ 'tflite-v1.15', 'armnn-v19.08' ]\n",
    "        for library in libraries:\n",
    "            if library == 'armnn-v19.08':\n",
    "                if platform == 'rpi4':\n",
    "                    backends = [ 'neon' ]\n",
    "                else:\n",
    "                    backends = [ 'neon', 'opencl' ]\n",
    "                library_backends = [ library+'-'+backend for backend in backends ]\n",
    "            else:\n",
    "                library_backends = [ library ]\n",
    "            for library_backend in library_backends:\n",
    "                division_system = division+'-'+platform+'-'+library_backend\n",
    "                frameworks = {\n",
    "                    'armnn-v19.08-opencl' : 'ArmNN v19.08 (OpenCL)',\n",
    "                    'armnn-v19.08-neon' : 'ArmNN v19.08 (Neon)',\n",
    "                    'tflite-v1.13' : 'TFLite v1.13.1',\n",
    "                    'tflite-v1.15' : 'TFLite v1.15.0-rc2',\n",
    "                }\n",
    "                template = deepcopy(platform_templates[platform])\n",
    "                template.update({\n",
    "                    'division'  : division,\n",
    "                    'framework' : frameworks[library_backend]\n",
    "                })\n",
    "                if not library_backend.endswith('opencl'):\n",
    "                    template.update({\n",
    "                        'accelerator_frequency' : '-',\n",
    "                        'accelerator_memory_capacity' : '-',\n",
    "                        'accelerator_model_name' : '-',\n",
    "                        'accelerators_per_node' : '0',\n",
    "                    })\n",
    "                division_systems[division_system] = template\n",
    "                print(\"=\" * 100)\n",
    "                print(division_system)\n",
    "                print(\"=\" * 100)\n",
    "                pprint(template)\n",
    "                print(\"-\" * 100)\n",
    "                print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"implementations\"></a>\n",
    "## Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate implementation_benchmarks dictionary.\n",
    "implementation_benchmarks = {}\n",
    "\n",
    "# Default `system_desc_id_imp.json` (to catch uninitialized descriptions)\n",
    "default_implementation_benchmark_json = {\n",
    "    \"input_data_types\": \"required\",\n",
    "    \"retraining\": \"required\",\n",
    "    \"starting_weights_filename\": \"required\",\n",
    "    \"weight_data_types\": \"required\",\n",
    "    \"weight_transformations\": \"required\"\n",
    "}\n",
    "\n",
    "# For each implementation.\n",
    "for implementation in [ 'image-classification-tflite', 'image-classification-armnn-tflite' ]:\n",
    "    # Add MobileNet.\n",
    "    implementation_mobilenet = implementation+'-'+'mobilenet'\n",
    "    implementation_benchmarks[implementation_mobilenet] = {\n",
    "        \"input_data_types\": \"fp32\",\n",
    "        \"weight_data_types\": \"fp32\",\n",
    "        \"retraining\": \"no\",\n",
    "        \"starting_weights_filename\": \"https://zenodo.org/record/2269307/files/mobilenet_v1_1.0_224.tgz\",\n",
    "        \"weight_transformations\": \"TFLite\"\n",
    "    }\n",
    "    # Add ResNet.\n",
    "    implementation_resnet = implementation+'-'+'resnet'\n",
    "    implementation_benchmarks[implementation_resnet] = {\n",
    "        \"input_data_types\": \"fp32\",\n",
    "        \"weight_data_types\": \"fp32\",\n",
    "        \"retraining\": \"no\",\n",
    "        \"starting_weights_filename\": \"https://zenodo.org/record/2535873/files/resnet50_v1.pb\",\n",
    "        \"weight_transformations\": \"TF -> TFLite\"\n",
    "    }\n",
    "    # Add any MobileNets-v1,v2 model.\n",
    "    def add_implementation_mobilenet(implementation_benchmarks, version, multiplier, resolution):\n",
    "        base_url = 'https://zenodo.org/record/2269307/files' if version == 1 else 'https://zenodo.org/record/2266646/files'\n",
    "        url = '{}/mobilenet_v{}_{}_{}.tgz'.format(base_url, version, multiplier, resolution)\n",
    "        benchmark = 'mobilenet-v{}-{}-{}'.format(version, multiplier, resolution)\n",
    "        if implementation == 'image-classification-tflite':\n",
    "            weights_transformations = 'TFLite'\n",
    "        elif implementation == 'image-classification-armnn-tflite':\n",
    "            weights_transformations = 'TFLite -> ArmNN'\n",
    "        else:\n",
    "            raise \"Unknown implementation '%s'!\" % implementation\n",
    "        implementation_benchmark = implementation+'-'+benchmark\n",
    "        implementation_benchmarks[implementation_benchmark] = {\n",
    "            \"input_data_types\": \"fp32\",\n",
    "            \"weight_data_types\": \"fp32\",\n",
    "            \"retraining\": \"no\",\n",
    "            \"starting_weights_filename\": url,\n",
    "            \"weight_transformations\": weights_transformations\n",
    "        }\n",
    "        return\n",
    "    # MobileNet-v1.\n",
    "    version = 1\n",
    "    for multiplier in [ 1.0, 0.75, 0.5, 0.25 ]:\n",
    "        for resolution in [ 224, 192, 160, 128 ]:\n",
    "            add_implementation_mobilenet(implementation_benchmarks, version, multiplier, resolution)\n",
    "    # MobileNet-v2.\n",
    "    version = 2\n",
    "    for multiplier in [ 1.0, 0.75, 0.5, 0.35 ]:\n",
    "        for resolution in [ 224, 192, 160, 128, 96 ]:\n",
    "            add_implementation_mobilenet(implementation_benchmarks, version, multiplier, resolution)\n",
    "    add_implementation_mobilenet(implementation_benchmarks, version=2, multiplier=1.3, resolution=224)\n",
    "    add_implementation_mobilenet(implementation_benchmarks, version=2, multiplier=1.4, resolution=224)\n",
    "\n",
    "# from pprint import pprint\n",
    "# pprint(implementation_benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "implementation_readmes = {}\n",
    "implementation_readmes['image-classification-tflite'] = \\\n",
    "\"\"\"# MLPerf Inference - Image Classification - TFLite\n",
    "\n",
    "This C++ implementation uses TFLite to run TFLite models for Image Classification on CPUs.\n",
    "\n",
    "## Links\n",
    "- [Jupyter notebook](https://nbviewer.jupyter.org/urls/dl.dropbox.com/s/1xlv5oacgobrfd4/mlpef-inference-v0.5-dividiti.ipynb)\n",
    "- [Source code](https://github.com/ctuning/ck-mlperf/tree/master/program/image-classification-tflite-loadgen).\n",
    "- [Instructions](https://github.com/mlperf/inference/blob/master/v0.5/classification_and_detection/optional_harness_ck/classification/tflite/README.md).\n",
    "\"\"\"\n",
    "\n",
    "implementation_readmes['image-classification-armnn-tflite'] = \\\n",
    "\"\"\"# MLPerf Inference - Image Classification - ArmNN-TFLite\n",
    "\n",
    "This C++ implementation uses ArmNN with the TFLite frontend to run TFLite models for Image Classification on Arm Cortex CPUs and Arm Mali GPUs.\n",
    "\n",
    "## Links\n",
    "- [Jupyter notebook](https://nbviewer.jupyter.org/urls/dl.dropbox.com/s/1xlv5oacgobrfd4/mlpef-inference-v0.5-dividiti.ipynb)\n",
    "- [Source code](https://github.com/ctuning/ck-mlperf/tree/master/program/image-classification-armnn-tflite-loadgen).\n",
    "- [Instructions](https://github.com/ARM-software/armnn-mlperf/blob/master/README.md).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "implementation_paths = {}\n",
    "for implementation in [ 'image-classification-tflite', 'image-classification-armnn-tflite' ]:\n",
    "    implementation_loadgen = implementation+'-loadgen'\n",
    "    r = ck.access({'action':'find', 'repo_uoa':'ck-mlperf', 'module_uoa':'program', 'data_uoa':implementation_loadgen})\n",
    "    if r['return']>0:\n",
    "        print('Error: %s' % r['error'])\n",
    "        exit(1)\n",
    "    implementation_paths[implementation] = r['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements_readmes = {}\n",
    "for division_upper in [ 'Closed', 'Open' ]:\n",
    "    division_lower = division_upper.lower()\n",
    "    measurements_readmes[division_lower] = '''# MLPerf Inference - {} Division - Image Classification\n",
    "\n",
    "We performed our measurements using automated, customizable, portable and reproducible\n",
    "[Collective Knowledge](http://cknowledge.org) workflows. Our workflows automatically\n",
    "install dependencies (models, datasets, etc.), preprocess input data in the correct way,\n",
    "and so on.\n",
    "\n",
    "## CK repositories\n",
    "\n",
    "As CK is always evolving, it is hard to pin particular revisions of all repositories.\n",
    "\n",
    "The most relevant repositories and their latest revisions on the submission date:\n",
    "- [ck-mlperf](https://github.com/ctuning/ck-mlperf) @ [ee77cfd](https://github.com/ctuning/ck-mlperf/commit/ee77cfd3ddfa30739a8c2f483fe9ba83a233a000) (contains programs integrated with LoadGen, model packages and scripts).\n",
    "- [ck-env](https://github.com/ctuning/ck-env) @ [f9ac337](https://github.com/ctuning/ck-env/commit/f9ac3372cdc82fa46b2839e45fc67848ab4bac03) (contains dataset descriptions, preprocessing methods, etc.)\n",
    "- [ck-tensorflow](https://github.com/ctuning/ck-tensorflow) @ [eff8bec](https://github.com/ctuning/ck-tensorflow/commit/eff8bec192021162e4a336dbd3e795afa30b7d26) (contains TFLite packages).\n",
    "- [armnn-mlperf](https://github.com/arm-software/armnn-mlperf) @ [42f44a2](https://github.com/ARM-software/armnn-mlperf/commit/42f44a266b6b4e04901255f46f6d34d12589208f) (contains ArmNN/ArmCL packages).\n",
    "\n",
    "## Links\n",
    "- [Bash script](https://github.com/ctuning/ck-mlperf/tree/master/script/mlperf-inference-v0.5.{}.image-classification) used to invoke benchmarking on Linux systems or Android devices.\n",
    "'''.format(division_upper, division_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snapshot of https://github.com/dividiti/inference/blob/61220457dec221ed1984c62bd9d382698bd71bc6/v0.5/mlperf.conf\n",
    "mlperf_conf_6122045 = '''\n",
    "# The format of this config file is 'key = value'.\n",
    "# The key has the format 'model.scenario.key'. Value is mostly int64_t.\n",
    "# Model maybe '*' as wildcard. In that case the value applies to all models.\n",
    "# All times are in milli seconds\n",
    "\n",
    "*.SingleStream.target_latency = 10\n",
    "*.SingleStream.target_latency_percentile = 90\n",
    "*.SingleStream.min_duration = 60000\n",
    "*.SingleStream.min_query_count = 1024\n",
    "\n",
    "*.MultiStream.target_qps = 20\n",
    "*.MultiStream.target_latency_percentile = 99\n",
    "*.MultiStream.samples_per_query = 4\n",
    "*.MultiStream.max_async_queries = 1\n",
    "*.MultiStream.target_latency = 50\n",
    "*.MultiStream.min_duration = 60000\n",
    "*.MultiStream.min_query_count = 270336\n",
    "ssd-resnet34.MultiStream.target_qps = 15\n",
    "ssd-resnet34.MultiStream.target_latency = 66\n",
    "gnmt.MultiStream.min_query_count = 90112\n",
    "gnmt.MultiStream.target_latency = 100\n",
    "gnmt.MultiStream.target_qps = 10\n",
    "gnmt.MultiStream.target_latency_percentile = 97\n",
    "\n",
    "*.Server.target_qps = 1.0\n",
    "*.Server.target_latency = 10\n",
    "*.Server.target_latency_percentile = 99\n",
    "*.Server.target_duration = 0\n",
    "*.Server.min_duration = 60000\n",
    "*.Server.min_query_count = 270336\n",
    "resnet50.Server.target_latency = 15\n",
    "ssd-resnet34.Server.target_latency = 100\n",
    "gnmt.Server.min_query_count = 90112\n",
    "gnmt.Server.target_latency = 250\n",
    "gnmt.Server.target_latency_percentile = 97\n",
    "\n",
    "*.Offline.target_qps = 1.0\n",
    "*.Offline.target_latency_percentile = 90\n",
    "*.Offline.min_duration = 60000\n",
    "*.Offline.min_query_count = 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get\"></a>\n",
    "## Get the experimental data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download experimental data and add CK repositories as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get_image_classification_closed\"></a>\n",
    "### Image Classification - Closed (MobileNet, ResNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `firefly`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ wget https://www.dropbox.com/s/3md826fk7k1taf3/mlperf.closed.image-classification.firefly.tflite-v1.15.zip\n",
    "$ ck add repo --zip=mlperf.closed.image-classification.firefly.tflite-v1.15.zip\n",
    "\n",
    "$ https://www.dropbox.com/s/jusoz329mhixpxm/mlperf.closed.image-classification.firefly.armnn-v19.08.neon.zip\n",
    "$ ck add repo --zip=mlperf.closed.image-classification.firefly.armnn-v19.08.neon.zip\n",
    "\n",
    "$ wget https://www.dropbox.com/s/08lzbz7jl2w5jhu/mlperf.closed.image-classification.firefly.armnn-v19.08.opencl.zip\n",
    "$ ck add repo --zip=mlperf.closed.image-classification.firefly.armnn-v19.08.opencl.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `hikey960`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ wget https://www.dropbox.com/s/lqnffl6wbaeceul/mlperf.closed.image-classification.hikey960.tflite-v1.15.zip\n",
    "$ ck add repo --zip=mlperf.closed.image-classification.hikey960.tflite-v1.15.zip\n",
    "\n",
    "$ wget https://www.dropbox.com/s/6m6uv1d33yc82f8/mlperf.closed.image-classification.hikey960.armnn-v19.08.neon.zip\n",
    "$ ck add repo --zip=mlperf.closed.image-classification.hikey960.armnn-v19.08.neon.zip\n",
    "\n",
    "$ wget https://www.dropbox.com/s/bz56y4damfqggr8/mlperf.closed.image-classification.hikey960.armnn-v19.08.opencl.zip\n",
    "$ ck add repo --zip=mlperf.closed.image-classification.hikey960.armnn-v19.08.opencl.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `rpi4`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ wget https://www.dropbox.com/s/ig97x9cqoxfs3ne/mlperf.closed.image-classification.rpi4.tflite-v1.15.zip\n",
    "$ ck add repo --zip=mlperf.closed.image-classification.rpi4.tflite-v1.15.zip\n",
    "\n",
    "$ wget https://www.dropbox.com/s/ohcuyes409h66tx/mlperf.closed.image-classification.rpi4.armnn-v19.08.neon.zip\n",
    "$ ck add repo --zip=mlperf.closed.image-classification.rpi4.armnn-v19.08.neon.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `mate10pro`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ wget https://www.dropbox.com/s/t2o2elqdyitqlpi/mlperf.closed.image-classification.mate10pro.armnn-v19.08.neon.zip\n",
    "$ ck add repo --zip=mlperf.closed.image-classification.mate10pro.armnn-v19.08.neon.zip\n",
    "\n",
    "$ wget https://www.dropbox.com/s/bi2owxxpcfm6n2s/mlperf.closed.image-classification.mate10pro.armnn-v19.08.opencl.zip\n",
    "$ ck add repo --zip=mlperf.closed.image-classification.mate10pro.armnn-v19.08.opencl.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** accuracy experiment with TFLite v1.15 for ResNet aborted (estimated 17 hours)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `mate10pro` (\"BAD LOADGEN\" - only for testing the checker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ wget https://www.dropbox.com/s/nts8e7unb7vm68f/mlperf.closed.image-classification.mate10pro.tflite-v1.13.mobilenet.BAD_LOADGEN.zip\n",
    "$ ck add repo --zip=mlperf.closed.image-classification.mate10pro.tflite-v1.13.mobilenet.BAD_LOADGEN.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get_image_classification_open\"></a>\n",
    "### Image Classification - Open (MobileNets-v1,v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `firefly`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ wget https://www.dropbox.com/s/q8ieqgnr3zn6w4y/mlperf.open.image-classification.firefly.tflite-v1.15.zip\n",
    "$ ck add repo --zip=mlperf.open.image-classification.firefly.tflite-v1.15.zip\n",
    "\n",
    "$ wget https://www.dropbox.com/s/3mmefvxc15m9o5b/mlperf.open.image-classification.firefly.armnn-v19.08.opencl.zip\n",
    "$ ck add repo --zip=mlperf.open.image-classification.firefly.armnn-v19.08.opencl.zip\n",
    "\n",
    "$ wget https://www.dropbox.com/s/hrupp4o4apo3dfa/mlperf.open.image-classification.firefly.armnn-v19.08.neon.zip\n",
    "$ ck add repo --zip=mlperf.open.image-classification.firefly.armnn-v19.08.neon.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `hikey960`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ wget https://www.dropbox.com/s/2gbbpsd2pjurvc8/mlperf.open.image-classification.hikey960.tflite-v1.15.zip\n",
    "$ ck add repo --zip=mlperf.open.image-classification.hikey960.tflite-v1.15.zip\n",
    "\n",
    "$ wget https://www.dropbox.com/s/m5illg8i2tse5hg/mlperf.open.image-classification.hikey960.armnn-v19.08.opencl.zip\n",
    "$ ck add repo --zip=mlperf.open.image-classification.hikey960.armnn-v19.08.opencl.zip\n",
    "\n",
    "$ wget https://www.dropbox.com/s/3cujqfe4ps0g66h/mlperf.open.image-classification.hikey960.armnn-v19.08.neon.zip\n",
    "$ ck add repo --zip=mlperf.open.image-classification.hikey960.armnn-v19.08.neon.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `rpi4`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ wget https://www.dropbox.com/s/awhdqjq3p4tre2q/mlperf.open.image-classification.rpi4.tflite-v1.15.zip\n",
    "$ ck add repo --zip=mlperf.open.image-classification.rpi4.tflite-v1.15.zip\n",
    "\n",
    "$ wget https://www.dropbox.com/s/0oketvqml7gyzl0/mlperf.open.image-classification.rpi4.armnn-v19.08.neon.zip\n",
    "$ ck add repo --zip=mlperf.open.image-classification.rpi4.armnn-v19.08.neon.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `mate10pro` (a perfectly valid closed submission - just a little bit late after the deadline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ wget https://www.dropbox.com/s/avi6h9m2demz5zr/mlperf.open.image-classification.mate10pro.tflite-v1.13.mobilenet.zip\n",
    "$ ck add repo --zip=mlperf.open.image-classification.mate10pro.tflite-v1.13.mobilenet.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"checklist\"></a>\n",
    "## Generate the submission checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checklist_template = \"\"\"MLPerf Inference 0.5 Self-Certification Checklist\n",
    "\n",
    "Name of Certifying Engineer(s): %(name)s\n",
    "\n",
    "Email of Certifying Engineer(s): %(email)s\n",
    "\n",
    "Name of System(s) Under Test: %(systems)s\n",
    "\n",
    "Division (check one):\n",
    "- [%(open)s] Open\n",
    "- [%(closed)s] Closed\n",
    "\n",
    "Category (check one):\n",
    "- [x] Available\n",
    "- [ ] Preview\n",
    "- [ ] Research, Development, and Internal (RDI)\n",
    "\n",
    "Benchmark (check one):\n",
    "- [%(mobilenet)s] MobileNet\n",
    "- [ ] SSD-MobileNet\n",
    "- [%(resnet)s] ResNet\n",
    "- [ ] SSD-1200\n",
    "- [ ] NMT\n",
    "- [%(other)s] Other, please specify:\n",
    "\n",
    "Please fill in the following tables adding lines as necessary:\n",
    "97%%-tile latency is required for NMT only. 99%%-tile is required for all other models.\n",
    "\n",
    "### Single Stream Results Table\n",
    "| SUT Name | Benchmark | Query Count | Accuracy |\n",
    "|----------|-----------|-------------|----------|\n",
    "| hikey960 | mobilenet | 50000       | 71.676%% |\n",
    "| rpi4     | mobilenet | 50000       | 71.676%% |\n",
    "| firefly  | mobilenet | 50000       | 71.676%% |\n",
    "| mate10pro| mobilenet | 50000       | 71.516%% |\n",
    "| hikey960 | mobilenet | 50000       | 76.522%% |\n",
    "| rpi4     | mobilenet | 50000       | 76.442%% |\n",
    "| firefly  | mobilenet | 50000       | 76.442%% |\n",
    "| mate10pro| mobilenet | 50000       | 28.926%% |\n",
    "\n",
    "### Multi-Stream Results Table\n",
    "| SUT Name | Benchmark | Query Count |  Accuracy | 97%%-tile Latency | 99%%-tile Latency |\n",
    "|----------|-----------|-------------|-----------|------------------|------------------|\n",
    "|          |           |             |           |                  |                  |\n",
    "\n",
    "### Server Results Table\n",
    "| SUT Name | Benchmark | Query Count | Accuracy | 97%%-tile Latency | 99%%-tile Latency |\n",
    "|----------|-----------|-------------|----------|------------------|------------------|\n",
    "|          |           |             |          |                  |                  |\n",
    "\n",
    "### Offline Results Table\n",
    "| SUT Name | Benchmark | Sample Count | Accuracy | \n",
    "|----------|-----------|--------------|----------|\n",
    "|          |           |              |          |\n",
    "\n",
    "Scenario (check all that apply):\n",
    "- [x] Single-Stream\n",
    "- [ ] Multi-Stream\n",
    "- [ ] Server\n",
    "- [ ] Offline\n",
    "\n",
    "For each SUT, does the submission meet the latency target for each\n",
    "combination of benchmark and scenario? (check all that apply)\n",
    "- [x] Yes (Single-Stream and Offline no requirements)\n",
    "- [ ] Yes (MobileNet x Multi-Stream 50 ms @ 99%%)\n",
    "- [ ] Yes (MobileNet x Server 10 ms @ 99%%)\n",
    "- [ ] Yes (SSD-MobileNet x Multi-Stream 50 ms @ 99%%)\n",
    "- [ ] Yes (SSD-MobileNet x Server 10 ms @ 99%%)\n",
    "- [ ] Yes (ResNet x Multi-Stream 50 ms @ 99%%)\n",
    "- [ ] Yes (ResNet x Server 15 ms @ 99%%)\n",
    "- [ ] Yes (SSD-1200 x Multi-Stream 66 ms @ 99%%).\n",
    "- [ ] Yes (SSD-1200 x Server 100 ms @ 99%%)\n",
    "- [ ] Yes (NMT x Multi-Stream 100 ms @ 97%%)\n",
    "- [ ] Yes (NMT x Server 250 ms @ 97%%)\n",
    "- [ ] No\n",
    "\n",
    "\n",
    "For each SUT, is the appropriate minimum number of queries or samples\n",
    "met, depending on the Scenario x Benchmark? (check all that apply)\n",
    "- [x] Yes (Single-Stream 1,024 queries)\n",
    "- [ ] Yes (Offline 24,576 samples)\n",
    "- [ ] Yes (NMT Server and Multi-Stream 90,112 queries)\n",
    "- [ ] Yes (Image Models Server and Multi-Stream 270,336 queries)\n",
    "- [ ] No\n",
    "\n",
    "For each SUT and scenario, is the benchmark accuracy target met?\n",
    "(check all that apply)\n",
    "- [%(mobilenet_accuracy_met)s] Yes (MobileNet 71.68%% x 98%%)\n",
    "- [ ] Yes (SSD-MobileNet 0.22 mAP x 99%%)\n",
    "- [%(resnet_accuracy_met)s] Yes (ResNet 76.46%% x 99%%)\n",
    "- [ ] Yes (SSD-1200 0.20 mAP x 99%%)\n",
    "- [ ] Yes (NMT 23.9 BLEU x 99%%)\n",
    "- [%(accuracy_not_met)s] No\n",
    "\n",
    "For each SUT and scenario, did the submission run on the whole\n",
    "validation set in accuracy mode? (check one)\n",
    "- [x] Yes\n",
    "- [ ] No\n",
    "\n",
    "How many samples are loaded into the QSL in performance mode?\n",
    "1024\n",
    "\n",
    "For each SUT and scenario, does the number of loaded samples in the\n",
    "QSL in performance mode meet the minimum requirement?  (check all that\n",
    "apply)\n",
    "- [x] Yes (ResNet and MobileNet 1,024 samples)\n",
    "- [ ] Yes (SSD-MobileNet 256 samples)\n",
    "- [ ] Yes (SSD-1200 64 samples)\n",
    "- [ ] Yes (NMT 3,903,900 samples)\n",
    "- [ ] No\n",
    "\n",
    "For each SUT and scenario, is the experimental duration greater than\n",
    "or equal to 60 seconds?  (check one)\n",
    "- [x] Yes\n",
    "- [ ] No\n",
    "\n",
    "Does the submission use LoadGen? (check one)\n",
    "- [x] Yes\n",
    "- [ ] No\n",
    "\n",
    "Is your loadgen commit from one of these allowed commit hashes?\n",
    "- [x] 61220457dec221ed1984c62bd9d382698bd71bc6\n",
    "- [ ] 5684c11e3987b614aae830390fa0e92f56b7e800\n",
    "- [ ] 55c0ea4e772634107f3e67a6d0da61e6a2ca390d\n",
    "- [ ] d31c18fbd9854a4f1c489ca1bc4cd818e48f2bc5\n",
    "- [ ] 1d0e06e54a7d763cf228bdfd8b1e987976e4222f\n",
    "- [x] Other, please specify:\n",
    "\n",
    "Do you have any additional change to LoadGen? (check one)\n",
    "- [ ] Yes, please specify:\n",
    "- [x] No\n",
    "\n",
    "Does the submission run the same code in accuracy and performance\n",
    "modes? (check one)\n",
    "- [x] Yes\n",
    "- [ ] No\n",
    "\n",
    "Where is the LoadGen trace stored? (check one)\n",
    "- [x] Host DRAM\n",
    "- [ ] Other, please specify:\n",
    "\n",
    "For the submitted result, what is the QSL random number generator seed?\n",
    "- [x] 0x2b7e151628aed2a6ULL (3133965575612453542)\n",
    "- [ ] Other, please specify:\n",
    "\n",
    "For the submitted results, what is the sample index random number generator seed?\n",
    "- [x] 0x093c467e37db0c7aULL (665484352860916858)\n",
    "- [ ] Other, please specify:\n",
    "\n",
    "For the submitted results, what is the schedule random number generator seed?\n",
    "- [x] 0x3243f6a8885a308dULL (3622009729038561421)\n",
    "- [ ] Other, please specify:\n",
    "\n",
    "For each SUT and scenario, is the submission run the correct number of\n",
    "times for the relevant scenario? (check one)\n",
    "- [x] Yes (Accuracy 1x Performance 1x Single-Stream, Multi-Stream,\n",
    "Offline)\n",
    "- [ ] Yes (Accuracy 1x Performance 5x Server)\n",
    "- [ ] No\n",
    "\n",
    "Are the weights calibrated using data outside of the calibration set?\n",
    "(check one)\n",
    "- [ ] Yes\n",
    "- [x] No\n",
    "\n",
    "What untimed pre-processing does the submission use? (check all that apply)\n",
    "- [x] Resize\n",
    "- [ ] Reorder channels or transpose\n",
    "- [ ] Pad\n",
    "- [x] A single crop\n",
    "- [x] Mean subtraction and normalization\n",
    "- [ ] Convert to whitelisted format\n",
    "- [ ] No pre-processing\n",
    "- [ ] Other, please specify:\n",
    "\n",
    "What numerics does the submission use? (check all that apply)\n",
    "- [ ] INT4\n",
    "- [ ] INT8\n",
    "- [ ] INT16\n",
    "- [ ] UINT8\n",
    "- [ ] UINT16\n",
    "- [ ] FP11\n",
    "- [ ] FP16\n",
    "- [ ] BF16\n",
    "- [x] FP32\n",
    "- [ ] Other, please specify:\n",
    "\n",
    "Which of the following techniques does the submission use? (check all that apply)\n",
    "- [ ] Wholesale weight replacement\n",
    "- [ ] Weight supplements\n",
    "- [ ] Discarding non-zero weight elements\n",
    "- [ ] Pruning\n",
    "- [ ] Caching queries\n",
    "- [ ] Caching responses\n",
    "- [ ] Caching intermediate computations\n",
    "- [ ] Modifying weights during the timed portion of an inference run\n",
    "- [ ] Weight quantization algorithms that are similar in size to the\n",
    "non-zero weights they produce\n",
    "- [ ] Hard coding the total number of queries\n",
    "- [ ] Techniques that boost performance for fixed length experiments but\n",
    "are inapplicable to long-running services except in the offline\n",
    "scenario\n",
    "- [ ] Using knowledge of the LoadGen implementation to predict upcoming\n",
    "lulls or spikes in the server scenario\n",
    "- [ ] Treating beams in a beam search differently. For example,\n",
    "employing different precision for different beams\n",
    "- [ ] Changing the number of beams per beam search relative to the reference\n",
    "- [ ] Incorporating explicit statistical information about the performance or accuracy sets\n",
    "- [ ] Techniques that take advantage of upsampled images.\n",
    "- [ ] Techniques that only improve performance when there are identical samples in a query.\n",
    "- [x] None of the above\n",
    "\n",
    "Is the submission congruent with all relevant MLPerf rules?\n",
    "- [x] Yes\n",
    "- [ ] No\n",
    "\n",
    "For each SUT, does the submission accurately reflect the real-world\n",
    "performance of the SUT?\n",
    "- [x] Yes\n",
    "- [ ] No\n",
    "\"\"\"\n",
    "\n",
    "# \"systems\": \"Raspberry Pi 4 (rpi4); Firefly RK3399 (firefly); Linaro HiKey960 (hikey960); Huawei Mate 10 Pro (mate10pro)\",\n",
    "def get_checklist(checklist_template=checklist_template, name=\"Anton Lokhmotov\", email=\"anton@dividiti.com\",\n",
    "                  system=\"Raspberry Pi 4 (rpi4)\", division_closed=True,\n",
    "                  mobilenet=True, resnet=False, other=False,\n",
    "                  mobilenet_accuracy_met=True, resnet_accuracy_met=True, accuracy_not_met=False):\n",
    "    def tick(var): return \"x\" if var else \" \"\n",
    "    print(\"=\" * 100)\n",
    "    print(system)\n",
    "    print(\"=\" * 100)\n",
    "    checklist = checklist_template % {\n",
    "        \"name\" : name,\n",
    "        \"email\" : email,\n",
    "        \"systems\": system,\n",
    "        # Division.\n",
    "        \"open\" : tick(not division_closed),\n",
    "        \"closed\" : tick(division_closed),\n",
    "        # Benchmark.\n",
    "        \"mobilenet\": tick(mobilenet),\n",
    "        \"resnet\": tick(resnet),\n",
    "        \"other\": tick(other),\n",
    "        # Accuracy.\n",
    "        \"mobilenet_accuracy_met\" : tick(mobilenet and mobilenet_accuracy_met),\n",
    "        \"resnet_accuracy_met\" : tick(resnet and resnet_accuracy_met),\n",
    "        \"accuracy_not_met\" : tick((resnet and not resnet_accuracy_met) or (mobilenet and not mobilenet_accuracy_met) or accuracy_not_met)\n",
    "    }\n",
    "    print(checklist)\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    return checklist\n",
    "\n",
    "null = get_checklist(system='Linaro HiKey 960 (hikey960)', mobilenet=False, resnet=True)\n",
    "null = get_checklist(system='Huawei Mate 10 Pro (mate10pro)', mobilenet=False, resnet=True, resnet_accuracy_met=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"check\"></a>\n",
    "## Check the experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Image Classification - Closed (MobileNet, ResNet).\n",
    "#\n",
    "repos_image_classification_closed = [\n",
    "    # firefly\n",
    "    'mlperf.closed.image-classification.firefly.tflite-v1.15', # https://github.com/mlperf/submissions_inference_0_5/pull/18\n",
    "    'mlperf.closed.image-classification.firefly.armnn-v19.08.neon', # https://github.com/mlperf/submissions_inference_0_5/pull/21\n",
    "    'mlperf.closed.image-classification.firefly.armnn-v19.08.opencl', #https://github.com/mlperf/submissions_inference_0_5/pull/22\n",
    "    # hikey960\n",
    "    'mlperf.closed.image-classification.hikey960.tflite-v1.15', # https://github.com/mlperf/submissions_inference_0_5/pull/23\n",
    "    'mlperf.closed.image-classification.hikey960.armnn-v19.08.neon', # https://github.com/mlperf/submissions_inference_0_5/pull/24\n",
    "    'mlperf.closed.image-classification.hikey960.armnn-v19.08.opencl', # https://github.com/mlperf/submissions_inference_0_5/pull/25\n",
    "    # rpi4\n",
    "    'mlperf.closed.image-classification.rpi4.tflite-v1.15', # https://github.com/mlperf/submissions_inference_0_5/pull/26/\n",
    "    'mlperf.closed.image-classification.rpi4.armnn-v19.08.neon', # https://github.com/mlperf/submissions_inference_0_5/pull/30\n",
    "    # mate10pro\n",
    "    'mlperf.closed.image-classification.mate10pro.armnn-v19.08.neon', # https://github.com/mlperf/submissions_inference_0_5/pull/32\n",
    "    'mlperf.closed.image-classification.mate10pro.armnn-v19.08.opencl', # https://github.com/mlperf/submissions_inference_0_5/pull/35\n",
    "]\n",
    "\n",
    "#\n",
    "# Image Classification - Open (MobileNets-v1,v2).\n",
    "#\n",
    "repos_image_classification_open = [\n",
    "    # firefly\n",
    "    'mlperf.open.image-classification.firefly.tflite-v1.15', # https://github.com/mlperf/submissions_inference_0_5/pull/39\n",
    "    'mlperf.open.image-classification.firefly.armnn-v19.08.opencl', # https://github.com/mlperf/submissions_inference_0_5/pull/40\n",
    "    'mlperf.open.image-classification.firefly.armnn-v19.08.neon', # ready for upload\n",
    "    # hikey960\n",
    "    'mlperf.open.image-classification.hikey960.tflite-v1.15', # https://github.com/mlperf/submissions_inference_0_5/pull/37\n",
    "    'mlperf.open.image-classification.hikey960.armnn-v19.08.opencl', # https://github.com/mlperf/submissions_inference_0_5/pull/38\n",
    "    'mlperf.open.image-classification.hikey960.armnn-v19.08.neon', # ready for upload\n",
    "    # rpi4\n",
    "    'mlperf.open.image-classification.rpi4.tflite-v1.15', # ready for upload\n",
    "    'mlperf.open.image-classification.rpi4.armnn-v19.08.neon', # ready for upload\n",
    "]\n",
    "\n",
    "repos = repos_image_classification_closed + repos_image_classification_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merged on the submission day.\n",
    "repos_image_classification_open_merged = [\n",
    "    # firefly\n",
    "    'mlperf.open.image-classification.firefly.tflite-v1.15', # https://github.com/mlperf/submissions_inference_0_5/pull/39\n",
    "    'mlperf.open.image-classification.firefly.armnn-v19.08.opencl', # https://github.com/mlperf/submissions_inference_0_5/pull/40\n",
    "    # hikey960\n",
    "    'mlperf.open.image-classification.hikey960.tflite-v1.15', # https://github.com/mlperf/submissions_inference_0_5/pull/37\n",
    "    'mlperf.open.image-classification.hikey960.armnn-v19.08.opencl', # https://github.com/mlperf/submissions_inference_0_5/pull/38\n",
    "]\n",
    "\n",
    "repos_merged = repos_image_classification_closed + repos_image_classification_open_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repos_for_testing = [\n",
    "#     # mate10pro - for testing the checker\n",
    "#     'mlperf.closed.image-classification.mate10pro.tflite-v1.13.mobilenet.BAD_LOADGEN', \n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ck recache repo\n",
    "for repo_uoa in repos:\n",
    "    print(\"=\" * 100)\n",
    "    print(repo_uoa)\n",
    "    print(\"=\" * 100)\n",
    "    !ck list $repo_uoa:experiment:* | sort\n",
    "    print(\"-\" * 100)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate upstream master.\n",
    "r = ck.access({'action':'locate', 'module_uoa':'env', 'tags':'mlperf,inference,source,upstream.master'})\n",
    "if r['return']>0:\n",
    "    print('Error: %s' % r['error'])\n",
    "    exit(1)\n",
    "# Pick any source location and look under 'inference/v0.5/mlperf.conf'.\n",
    "upstream_path = os.path.join(list(r['install_locations'].values())[0], 'inference')\n",
    "upstream_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_experimental_results(repo_uoa, module_uoa='experiment', tags='mlperf', submitter='dividiti', path=None):\n",
    "    if not path:\n",
    "        path_list = !ck find repo:$repo_uoa\n",
    "        path = path_list[0]\n",
    "    root_dir = os.path.join(path, 'submissions_inference_0_5')\n",
    "    if not os.path.exists(root_dir): os.mkdir(root_dir)\n",
    "    print(\"Storing results under '%s'\" % root_dir)\n",
    "    \n",
    "    r = ck.access({'action':'search', 'repo_uoa':repo_uoa, 'module_uoa':module_uoa, 'tags':tags})\n",
    "    if r['return']>0:\n",
    "        print('Error: %s' % r['error'])\n",
    "        exit(1)\n",
    "    experiments = r['lst']\n",
    "\n",
    "    dfs = []\n",
    "    for experiment in experiments:\n",
    "        data_uoa = experiment['data_uoa']\n",
    "        r = ck.access({'action':'list_points', 'repo_uoa':repo_uoa, 'module_uoa':module_uoa, 'data_uoa':data_uoa})\n",
    "        if r['return']>0:\n",
    "            print('Error: %s' % r['error'])\n",
    "            exit(1)\n",
    "        print(\"*\" * 100)\n",
    "\n",
    "        tags = r['dict']['tags']\n",
    "        #print(tags)\n",
    "        backend = ''\n",
    "        if 'accuracy' in tags:\n",
    "            if 'neon' in tags or 'opencl' in tags:\n",
    "                # Expected format: [ \"mlperf\", \"open\", \"image-classification\", \"firefly\", \"armnn-v19.08\", \"neon\", \"mobilenet-v1-0.5-128\", \"singlestream\", \"accuracy\", \"using-opencv\" ]\n",
    "                (_, division, task, platform, library, backend, benchmark, scenario, mode, preprocessing) = tags\n",
    "            else:\n",
    "                # Expected format: [ \"mlperf\", \"open\", \"image-classification\", \"firefly\", \"tflite-v1.15\", \"mobilenet-v1-0.5-128\", \"singlestream\", \"accuracy\", \"using-opencv\" ]\n",
    "                (_, division, task, platform, library, benchmark, scenario, mode, preprocessing) = tags\n",
    "        elif 'performance' in tags:            \n",
    "            if 'neon' in tags or 'opencl' in tags:\n",
    "                # Expected format: [ \"mlperf\", \"open\", \"image-classification\", \"firefly\", \"armnn-v19.08\", \"neon\", \"mobilenet-v1-0.5-128\", \"singlestream\", \"performance\" ]\n",
    "                (_, division, task, platform, library, backend, benchmark, scenario, mode) = tags\n",
    "            else:\n",
    "                # Expected format: [ \"mlperf\", \"open\", \"image-classification\", \"firefly\", \"tflite-v1.15\", \"mobilenet-v1-0.5-128\", \"singlestream\", \"performance\" ]\n",
    "                (_, division, task, platform, library, benchmark, scenario, mode) = tags\n",
    "        else:\n",
    "            raise \"Expected 'accuracy' or 'performance' in tags!\"\n",
    "            \n",
    "        organization = submitter\n",
    "\n",
    "        if backend != '':\n",
    "            system = platform+'-'+library+'-'+backend\n",
    "        else:\n",
    "            system = platform+'-'+library\n",
    "        division_system = division+'-'+system\n",
    "\n",
    "        if library.startswith('tflite'):\n",
    "            implementation = task+'-tflite'\n",
    "        elif library.startswith('armnn'):\n",
    "            implementation = task+'-armnn-tflite'\n",
    "        else: # Official app with CK adaptations.\n",
    "            implementation = 'mlperf-inference-vision'\n",
    "        implementation_benchmark = implementation+'-'+benchmark\n",
    "        \n",
    "        #\n",
    "        # Directory structure according to the Inference section of the General MLPerf Submission Rules:\n",
    "        # https://github.com/mlperf/policies/blob/master/submission_rules.adoc#552-inference\n",
    "        #\n",
    "        # <division>/\n",
    "        #   <organization>/\n",
    "        #\n",
    "        division_dir = os.path.join(root_dir, division)\n",
    "        if not os.path.exists(division_dir): os.mkdir(division_dir)\n",
    "        organization_dir = os.path.join(division_dir, organization)\n",
    "        if not os.path.exists(organization_dir): os.mkdir(organization_dir)\n",
    "        \n",
    "        #\n",
    "        #     \"systems\"/\n",
    "        #       <system_desc_id>.json\n",
    "        #\n",
    "        systems_dir = os.path.join(organization_dir, 'systems')\n",
    "        if not os.path.exists(systems_dir): os.mkdir(systems_dir)\n",
    "        system_json_name = '%s.json' % system\n",
    "        system_json_path = os.path.join(systems_dir, system_json_name)\n",
    "        with open(system_json_path, 'w') as system_json_file:\n",
    "#             pprint(division_system)\n",
    "#             pprint(division_systems)\n",
    "            system_json = division_systems.get(division_system, default_system_json)\n",
    "            json.dump(system_json, system_json_file, indent=2)\n",
    "            print('%s' % systems_dir)\n",
    "            if system_json == default_system_json:\n",
    "                print('  |_ %s [DEFAULT]' % system_json_name)\n",
    "                raise\n",
    "            else:\n",
    "                print('  |_ %s [%s]' % (system_json_name, division_system))\n",
    "        \n",
    "        #\n",
    "        #     \"code\"/\n",
    "        #       <benchmark_name_per_reference>/\n",
    "        #         <implementation_id>/\n",
    "        #           <Code interface with loadgen and other arbitrary stuff>\n",
    "        #\n",
    "        code_dir = os.path.join(organization_dir, 'code')\n",
    "        if not os.path.exists(code_dir): os.mkdir(code_dir)\n",
    "        # FIXME: For now, not always \"per reference\".\n",
    "        benchmark_dir = os.path.join(code_dir, benchmark)\n",
    "        if not os.path.exists(benchmark_dir): os.mkdir(benchmark_dir)\n",
    "\n",
    "        # Write checklist for each benchmark.\n",
    "        checklist_name = 'submission_checklist.txt'\n",
    "        checklist_path = os.path.join(benchmark_dir, checklist_name)\n",
    "        if(benchmark == 'mobilenet'):\n",
    "            checklist = get_checklist(system='Huawei Mate 10 Pro (mate10pro)')\n",
    "        elif(benchmark == 'resnet'):\n",
    "            checklist = get_checklist(system='Huawei Mate 10 Pro (mate10pro)',\n",
    "                                      mobilenet=False, resnet=True, resnet_accuracy_met=False)\n",
    "        else:\n",
    "            checklist = ''\n",
    "        with open(checklist_path, 'w') as checklist_file:\n",
    "            checklist_file.writelines(checklist)\n",
    "            \n",
    "        implementation_dir = os.path.join(benchmark_dir, implementation)\n",
    "        if not os.path.exists(implementation_dir): os.mkdir(implementation_dir)\n",
    "        print('%s' % code_dir)\n",
    "\n",
    "        # Create 'README.md'.\n",
    "        implementation_readme_name = 'README.md'\n",
    "        implementation_readme_path = os.path.join(implementation_dir, implementation_readme_name)\n",
    "#         pprint(implementation)\n",
    "#         pprint(implementation_readmes)\n",
    "        implementation_readme = implementation_readmes.get(implementation, '')\n",
    "        with open(implementation_readme_path, 'w') as implementation_readme_file:\n",
    "            implementation_readme_file.writelines(implementation_readme)\n",
    "        if implementation_readme == '':\n",
    "            print('  |_ %s [EMPTY]' % implementation_readme_name)\n",
    "            raise\n",
    "        else:\n",
    "            print('  |_ %s' % implementation_readme_name)\n",
    "        # TODO: Copy run.sh\n",
    "\n",
    "        #\n",
    "        #     \"measurements\"/\n",
    "        #       <system_desc_id>/\n",
    "        #         <benchmark>/\n",
    "        #           <scenario>/\n",
    "        #             <system_desc_id>_<implementation_id>.json\n",
    "        #             README.md\n",
    "        #             user.conf\n",
    "        #             mlperf.conf\n",
    "        #             calibration_process.adoc (?)\n",
    "        #\n",
    "        measurements_dir = os.path.join(organization_dir, 'measurements')\n",
    "        if not os.path.exists(measurements_dir): os.mkdir(measurements_dir)\n",
    "        system_dir = os.path.join(measurements_dir, system)\n",
    "        if not os.path.exists(system_dir): os.mkdir(system_dir)\n",
    "        benchmark_dir = os.path.join(system_dir, benchmark)\n",
    "        if not os.path.exists(benchmark_dir): os.mkdir(benchmark_dir)\n",
    "        scenario_dir = os.path.join(benchmark_dir, scenario)\n",
    "        if not os.path.exists(scenario_dir): os.mkdir(scenario_dir)\n",
    "        print(scenario_dir)\n",
    "        \n",
    "        # Create '<system_desc_id>_<implementation_id>.json'.\n",
    "        system_implementation_json_name = system+'_'+implementation+'.json'\n",
    "        system_implementation_json_path = os.path.join(scenario_dir, system_implementation_json_name)\n",
    "        with open(system_implementation_json_path, 'w') as system_implementation_json_file:\n",
    "            implementation_benchmark_json = implementation_benchmarks.get(implementation_benchmark, default_implementation_benchmark_json)\n",
    "            if implementation_benchmark_json != default_implementation_benchmark_json:\n",
    "                print('  |_ %s [for %s]' % (system_implementation_json_name, implementation_benchmark))\n",
    "                json.dump(implementation_benchmark_json, system_implementation_json_file, indent=2)\n",
    "            else:\n",
    "                print('  |_ %s [DEFAULT]' % system_implementation_json_name)\n",
    "                raise \"Default implementation!\"\n",
    "        \n",
    "        # Create 'README.md' based on the division (basically, mentions a division-specific script).\n",
    "        measurements_readme_name = 'README.md'\n",
    "        measurements_readme_path = os.path.join(scenario_dir, measurements_readme_name)\n",
    "        measurements_readme = measurements_readmes.get(division, '')\n",
    "        if measurements_readme != '':\n",
    "            with open(measurements_readme_path, 'w') as measurements_readme_file:\n",
    "                measurements_readme_file.writelines(measurements_readme)\n",
    "            print('  |_ %s [for %s division]' % (measurements_readme_name, division))\n",
    "        else:\n",
    "            raise \"Invalid measurements readme!\"\n",
    "        \n",
    "        # Copy 'user.conf' from implementation source.\n",
    "        user_conf_name = 'user.conf'\n",
    "        implementation_path = implementation_paths.get(implementation, '')\n",
    "        if implementation_path != '':\n",
    "            user_conf_path = os.path.join(implementation_path, user_conf_name)\n",
    "            copy2(user_conf_path, scenario_dir)\n",
    "            print('  |_ %s [from %s]' % (user_conf_name, user_conf_path))\n",
    "        else:\n",
    "            raise \"Invalid implementation path!\"\n",
    "        \n",
    "        # Copy 'mlperf.conf' from MLPerf Inference source.\n",
    "        mlperf_conf_name = 'mlperf.conf'\n",
    "        mlperf_conf_path = os.path.join(scenario_dir, mlperf_conf_name)\n",
    "        if implementation in [ 'image-classification-tflite', 'image-classification-armnn-tflite' ]:\n",
    "            # Write a snapshot from https://github.com/dividiti/inference/blob/61220457dec221ed1984c62bd9d382698bd71bc6/v0.5/mlperf.conf\n",
    "            with open(mlperf_conf_path, 'w') as mlperf_conf_file:\n",
    "                mlperf_conf_file.writelines(mlperf_conf_6122045)\n",
    "            print('  |_ %s [from %s]' % (mlperf_conf_name, 'github.com/mlperf/inference@6122045'))\n",
    "        else:\n",
    "            upstream_mlperf_conf_path = os.path.join(upstream_path, 'v0.5', 'mlperf.conf')\n",
    "            copy2(upstream_mlperf_conf_path, mlperf_conf_path)\n",
    "            print('  |_ %s [from %s]' % (mlperf_conf_name, upstream_mlperf_conf_path))\n",
    "        \n",
    "        #\n",
    "        #     \"results\"/\n",
    "        #       <system_desc_id>/\n",
    "        #         <benchmark>/\n",
    "        #           <scenario>/\n",
    "        #             performance/\n",
    "        #               run_x/ # 1 run for single stream and offline, 5 otherwise\n",
    "        #                 mlperf_log_summary.txt\n",
    "        #                 mlperf_log_detail.txt\n",
    "        #                 mlperf_log_trace.json\n",
    "        #             accuracy/\n",
    "        #               mlperf_log_accuracy.json\n",
    "        #         compliance_checker_log.txt\n",
    "        #\n",
    "        results_dir = os.path.join(organization_dir, 'results')\n",
    "        if not os.path.exists(results_dir): os.mkdir(results_dir)\n",
    "        system_dir = os.path.join(results_dir, system)\n",
    "        if not os.path.exists(system_dir): os.mkdir(system_dir)\n",
    "        benchmark_dir = os.path.join(system_dir, benchmark)\n",
    "        if not os.path.exists(benchmark_dir): os.mkdir(benchmark_dir)\n",
    "        scenario_dir = os.path.join(benchmark_dir, scenario)\n",
    "        if not os.path.exists(scenario_dir): os.mkdir(scenario_dir)\n",
    "        mode_dir = os.path.join(scenario_dir, mode)\n",
    "        if not os.path.exists(mode_dir): os.mkdir(mode_dir)\n",
    "        # For each point (should be one point for each performance run).\n",
    "        points = r['points']\n",
    "        for (point, point_idx) in zip(points, range(1,len(points)+1)):\n",
    "            point_file_path = os.path.join(r['path'], 'ckp-%s.0001.json' % point)\n",
    "            with open(point_file_path) as point_file:\n",
    "                point_data_raw = json.load(point_file)\n",
    "            characteristics_list = point_data_raw['characteristics_list']\n",
    "            characteristics = characteristics_list[0]\n",
    "            # Set the leaf directory.\n",
    "            if mode == 'performance':\n",
    "                run_dir = os.path.join(mode_dir, 'run_%d' % point_idx)\n",
    "                if not os.path.exists(run_dir): os.mkdir(run_dir)\n",
    "                last_dir = run_dir\n",
    "            else:\n",
    "                last_dir = mode_dir\n",
    "            print(last_dir)\n",
    "            # Dump files in the leaf directory.\n",
    "            mlperf_log = characteristics['run'].get('mlperf_log',{})\n",
    "            # Summary file (with errors and warnings in accuracy mode, with statistics in performance mode).\n",
    "            summary_txt_name = 'mlperf_log_summary.txt'\n",
    "            summary_txt_path = os.path.join(last_dir, summary_txt_name)\n",
    "            with open(summary_txt_path, 'w') as summary_txt_file:\n",
    "                summary_txt_file.writelines(mlperf_log.get('summary',''))\n",
    "                print('  |_ %s' % summary_txt_name)\n",
    "            # Detail file (with settings).\n",
    "            detail_txt_name = 'mlperf_log_detail.txt'\n",
    "            detail_txt_path = os.path.join(last_dir, detail_txt_name)\n",
    "            with open(detail_txt_path, 'w') as detail_txt_file:\n",
    "                detail_txt_file.writelines(mlperf_log.get('detail',''))\n",
    "                print('  |_ %s' % detail_txt_name)\n",
    "            # Accuracy file (with accuracy dictionary).\n",
    "            # FIXME: Move the next 5 lines into the (if mode == 'accuracy') block,\n",
    "            # once the submission checker no longer complains as follows:\n",
    "            # \"performance/run_1 has file list mismatch (['mlperf_log_accuracy.json'])\"\n",
    "            accuracy_json_name = 'mlperf_log_accuracy.json'\n",
    "            accuracy_json_path = os.path.join(last_dir, accuracy_json_name)\n",
    "            with open(accuracy_json_path, 'w') as accuracy_json_file:\n",
    "                json.dump(mlperf_log.get('accuracy',{}), accuracy_json_file, indent=2)\n",
    "                print('  |_ %s' % accuracy_json_name)\n",
    "            if mode == 'accuracy':\n",
    "                accuracy_imagenet_py = os.path.join(upstream_path, 'v0.5', 'classification_and_detection', 'tools', 'accuracy-imagenet.py')\n",
    "                imagenet_val_file = '$HOME/CK_TOOLS/dataset-imagenet-ilsvrc2012-aux/val.txt' # FIXME: Do not hardcode - locate via CK.\n",
    "                accuracy_txt_name = 'accuracy.txt'\n",
    "                accuracy_txt_path = os.path.join(last_dir, accuracy_txt_name)\n",
    "                accuracy_txt = !python3 $accuracy_imagenet_py --imagenet-val-file $imagenet_val_file --mlperf-accuracy-file $accuracy_json_path\n",
    "                with open(accuracy_txt_path, 'w') as accuracy_txt_file:\n",
    "                    accuracy_txt_file.writelines(accuracy_txt)\n",
    "                    # Print the first line containing accuracy info.\n",
    "                    print('  |_ %s [\"%s\"]' % (accuracy_txt_name, accuracy_txt[0]))\n",
    "#             # Trace file (should omit trace from v0.5).\n",
    "#             trace_json_name = 'mlperf_log_trace.json'\n",
    "#             trace_json_path = os.path.join(last_dir, trace_json_name)\n",
    "#             with open(trace_json_path, 'w') as trace_json_file:\n",
    "#                 json.dump(mlperf_log.get('trace',{}), trace_json_file, indent=2)    \n",
    "    \n",
    "    print(\"*\" * 100)\n",
    "    submission_checker_py = os.path.join(upstream_path, 'v0.5', 'tools', 'submission', 'submission-checker.py')\n",
    "    \n",
    "    # The checker has a weird bug. When submitting to open, 'closed/<organization>/results' must exist on disk.\n",
    "    # Vice versa, When submitting to closed, 'open/<organization>/results' must exist on disk. \n",
    "    # Therefore, create both directories if they do not exist before invoking the checker.\n",
    "    open_org_results_dir = os.path.join(root_dir, 'open', organization, 'results')\n",
    "    closed_org_results_dir = os.path.join(root_dir, 'closed', organization, 'results')\n",
    "    !mkdir -p $open_org_results_dir\n",
    "    !mkdir -p $closed_org_results_dir\n",
    "    # Run the checker.\n",
    "    checker_log = !python3 $submission_checker_py --input $root_dir --submitter $submitter\n",
    "    checker_log = \"\\n\".join(checker_log)\n",
    "    print(checker_log)\n",
    "    checker_log_name = 'compliance_checker_log.txt'\n",
    "    checker_log_path = os.path.join(system_dir, checker_log_name)\n",
    "    with open(checker_log_path, 'w') as checker_log_file:\n",
    "        checker_log_file.writelines(checker_log)\n",
    "        print(organization_dir)\n",
    "        print('  |_%s' % checker_log_name)\n",
    "\n",
    "    return\n",
    "\n",
    "# repos = [\n",
    "#     'mlperf.closed.image-classification.mate10pro.armnn-v19.08.neon'\n",
    "# ]\n",
    "repos = repos_merged\n",
    "for repo_uoa in repos:\n",
    "    # The path is where mlperf/submissions_inference_0_5 is cloned under.\n",
    "    check_experimental_results(repo_uoa, path='/home/anton/projects/mlperf/')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
